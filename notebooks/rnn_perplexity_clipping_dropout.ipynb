{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from rnn_lm import LanguageModel, create_dataset, loss_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'simple-examples/data/ptb.train.txt'\n",
    "valid_file = 'simple-examples/data/ptb.valid.txt'\n",
    "vocab_file = 'simple-examples/data/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify model params\n",
    "V = int(vocab_table.size())\n",
    "cell = 'lstm'\n",
    "d = 128\n",
    "h = 128\n",
    "\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(V=V, cell=cell, d=d, h=h)\n",
    "train_dataset = create_dataset(train_file, batch_size=BATCH_SIZE, vocab_table=vocab_table)\n",
    "valid_dataset = create_dataset(valid_file, batch_size=BATCH_SIZE, vocab_table=vocab_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: Perplexity\n",
    "\n",
    "<img src=\"ppl@2x.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "\n",
    "1. Compute average loss over the **entire** dataset\n",
    "2. Perplexity is $e^{L}$\n",
    "\n",
    "**Question**: What should be the perplexity for an untrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tgt_untrained = 1/10000\n",
    "loss_untrained = -np.log(p_tgt_untrained)\n",
    "print(f'loss: {loss_untrained}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = np.exp(loss_untrained)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model, dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(dataset):\n",
    "        num_words = int(tf.reduce_sum(datum[2]))\n",
    "        avg_loss = loss_fun(model, datum)\n",
    "        total_loss = avg_loss * num_words\n",
    "        total_words += num_words\n",
    "        if batch_num % 50 == 0:\n",
    "            print(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / float(num_words)\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ppl(lm, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load our saved model, which was trained for some steps, and see if it does any better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'lm'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=lm, optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ppl(lm, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does perplexity depend on the size of vocabulary in a language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: Gradient Clipping\n",
    "\n",
    "A key challenge in RNN is that sometimes gradients are too large. Pascanu, Tomas Mikolov \\& Bengio [suggested a simple fix](https://arxiv.org/abs/1211.5063) for the problem. If gradient is too large, clip it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(grads_and_vars, clip_ratio):\n",
    "  gradients, variables = zip(*grads_and_vars)\n",
    "  clipped, _ = tf.clip_by_global_norm(gradients, clip_ratio)\n",
    "  return zip(clipped, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grads_fun = tfe.implicit_value_and_gradients(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(V=V, cell=cell, d=d, h=h)\n",
    "checkpoint_dir = 'lm'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=lm, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "STATS_STEPS = 50\n",
    "EVAL_STEPS = 500\n",
    "\n",
    "valid_ppl = compute_ppl(lm, valid_dataset)\n",
    "print(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    for step_num, datum in enumerate(train_dataset, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun(lm, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl(lm, valid_dataset)\n",
    "            \n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "                \n",
    "        \n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3: Dropout\n",
    "\n",
    "An important regularization technique for RNN is to apply dropout\n",
    "\n",
    "* Randomly make some units zero\n",
    "* scale up remaining units so that signal length remains same!\n",
    "* Only applied at **train** time\n",
    "\n",
    "Let us check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = next(iter(train_dataset))\n",
    "word_vectors = lm.word_embedding(datum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at say 10th example, 8th word and first 4 features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[10][8][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_dropout = tf.nn.dropout(word_vectors, keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_dropout[10][8][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us change our Language Model to include dropout...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_lm import Embedding, StaticRNN\n",
    "class LanguageModel(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.word_embedding = Embedding(V, d)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=V)\n",
    "        \n",
    "    def call(self, datum, train=False, dropout=0.):\n",
    "        word_vectors = self.word_embedding(datum[0])\n",
    "        if train:\n",
    "            word_vectors = tf.nn.dropout(word_vectors, keep_prob=1-dropout)\n",
    "        rnn_outputs_time = self.rnn(word_vectors, datum[2])\n",
    "        \n",
    "        #We want to convert it back to shape batch_size x TimeSteps x h\n",
    "        rnn_outputs = tf.stack(rnn_outputs_time, axis=1)\n",
    "        if train:\n",
    "            rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob=1-dropout)\n",
    "        logits = self.output_layer(rnn_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(model, datum, train=False):\n",
    "    logits = model(datum, train)\n",
    "    mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[1]) * mask\n",
    "    return tf.reduce_sum(loss) / tf.cast(tf.reduce_sum(datum[2]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model, dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(dataset):\n",
    "        num_words = int(tf.reduce_sum(datum[2]))\n",
    "        avg_loss = loss_fun(model, datum)\n",
    "        total_loss = avg_loss * num_words\n",
    "        total_words += num_words\n",
    "        if batch_num % 50 == 0:\n",
    "            print(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / float(num_words)\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(model, datum):\n",
    "    return loss_fun(model, datum, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grads_fun = tfe.implicit_value_and_gradients(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(V=V, cell=cell, d=d, h=h)\n",
    "checkpoint_dir = 'lm'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=lm, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ppl(lm, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "STATS_STEPS = 50\n",
    "EVAL_STEPS = 500\n",
    "\n",
    "valid_ppl = compute_ppl(lm, valid_dataset)\n",
    "print(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    for step_num, datum in enumerate(train_dataset, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun(lm, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl(lm, valid_dataset)\n",
    "            \n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "                \n",
    "        \n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The code has been adapted from the [official tutorial on using eager for LM](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/rnn_ptb/rnn_ptb.py)\n",
    "\n",
    "In this notebook, we will explore how to build a **Neural Language Model**. Rather than directly showing code for the NLM, we will arrive at it step-by-step by discussing all key components. \n",
    "\n",
    "We will also leverage **tf.data** to build our data pipeline, something we found to be missing in the official tutorial.\n",
    "\n",
    "### P1: Enable Eager execution\n",
    "* We use `tfe` to add variables\n",
    "* enable_eager_execution() should be the first command in your notebook. Note that executing this again throws an error! Restart your notebook kernel to re-execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: Fixed random seed\n",
    "* A fixed random seed is required to reproduce your experiments!\n",
    "* This can help you debug your code!\n",
    "* You can select any number of your choice. We selected 42, any guesses why? :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3: Embedding Model\n",
    "Let us begin by building an **Embedding Model**. The job of embedding model is simple: Given a tensor of word indexes, return corresponding vectors (or rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, V, d):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "    \n",
    "    def call(self, word_indexes):\n",
    "        return tf.nn.embedding_lookup(self.W, word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give it a try by finding embeddings for word indexes: 5 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = Embedding(5000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128)\n"
     ]
    }
   ],
   "source": [
    "vecs = word_embeddings([5, 100])\n",
    "print(vecs.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = word_embeddings([[5, 100, 40], [2, 300, 90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 128)\n"
     ]
    }
   ],
   "source": [
    "print(vecs.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4: RNN Cell...\n",
    "\n",
    "We now have the ability to feed vectors for each time step. Now let us say we see two words and want to predict the third word in a sentence. We need a mechanism that can **summarize** all the words seen so far, and use the **summary** to generate a probability distribution for the next word.\n",
    "\n",
    "**Recurrent Neural Network(RNN)** does precisely that: It maintains a lossy summary of the inputs seen so far!\n",
    "\n",
    "<img src=\"recurrent_eqn@2x.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume we have a batch of 2 sentences, each sentence has 3 words. \n",
    "\n",
    "We will come to how RNN will handle variable length sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = [[20, 30, 400], [500, 0, 3]]\n",
    "word_vectors = word_embeddings(word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What should be shape of word_vectors? Recall em returns vectors of size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 128)\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we will not be able to pass the word_vectors directly. RNN proceses inputs **one time step** at a time!\n",
    "\n",
    "Enter, [tf.unstack](https://www.tensorflow.org/api_docs/python/tf/unstack)\n",
    "![title](tf.unstack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vectors_time: len:3 Shape[0]: (2, 128)\n"
     ]
    }
   ],
   "source": [
    "word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "print(f'word_vectors_time: len:{len(word_vectors_time)} Shape[0]: {word_vectors_time[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n"
     ]
    }
   ],
   "source": [
    "cell = tf.nn.rnn_cell.BasicRNNCell(256)\n",
    "init_state = cell.zero_state(batch_size=int(word_vectors.shape[0]), dtype=tf.float32)\n",
    "output, state = cell(word_vectors_time[0], init_state)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You might be wondering: We only talked about hidden state $h_t$ till now, why do we have two vectors being computed output and state. \n",
    "\n",
    "* For a BasicRNNCell output and state are identical. \n",
    "\n",
    "* For LSTM and GRU they have different meaning. All we need to understand is that it uses state and output to do its magic of being able to maintain and learn long term dependencies. \n",
    "\n",
    "* We would mostly use state to pass it to next time step, and output to make predictions at that time step.\n",
    "\n",
    "* Read this [excellent blog post on LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), in case you are interested in how LSTM works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5: RNN Model\n",
    "Now, we have all the pieces to build an RNN Model. Let us see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(RNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, word_vectors):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs = []\n",
    "        \n",
    "        state = self.cell.zero_state(batch_size=int(word_vectors.shape[0]), dtype=tf.float32)\n",
    "        for word_vector_time in word_vectors_time:\n",
    "            output, state = self.cell(word_vector_time, state)\n",
    "            outputs.append(output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num outputs: 3 Shape[0]: (2, 128)\n"
     ]
    }
   ],
   "source": [
    "word_indexes = [[20, 30, 400], [500, 0, 3]]\n",
    "word_vectors = word_embeddings(word_indexes)\n",
    "\n",
    "rnn = RNN(128, 'rnn')\n",
    "rnn_outputs = rnn(word_vectors)\n",
    "\n",
    "# Prints \"Num outputs: 3 Shape[0]: (2, 128)\"\n",
    "print(f'Num outputs: {len(rnn_outputs)} Shape[0]: {rnn_outputs[0].numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P6: Data pipeline\n",
    "\n",
    "We will work with a standard LM dataset: PTB dataset from Tomas Mikolov's webpage:\n",
    "```bash\n",
    "wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "tar xvf simple-examples.tgz\n",
    "```\n",
    "\n",
    "The first thing, we do with any data is to take a peek at it. \n",
    "\n",
    "```bash\n",
    "head -3 simple-examples/data/ptb.train.txt\n",
    "```\n",
    "\n",
    "```\n",
    "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
    "pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
    "mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
    "```\n",
    "Some key points to note:\n",
    "\n",
    "* We see here that there is a $<unk>$ token already.\n",
    "* There also seems another token $N$. This identifies a number. \n",
    "* Rest all words seem to be lower cased\n",
    "    \n",
    "Let us count up the vocab quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'simple-examples/data/ptb.train.txt'\n",
    "UNK='<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentences_file):\n",
    "    counter = {}\n",
    "    for sentence in open(sentences_file):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique words: 9999\n"
     ]
    }
   ],
   "source": [
    "counter = count_words(train_file)\n",
    "print(f'Num unique words: {len(counter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a special token EOS which signifies end of sentence. We add this to out vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write the vocab to file. Since, we are using OrderedDict, we will get words in order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(counter, vocab_file, unk=UNK, eos=EOS):\n",
    "    del counter[unk]\n",
    "    with open(vocab_file, 'w') as fw:\n",
    "        fw.write(f'{unk}\\n')\n",
    "        fw.write(f'{eos}\\n')\n",
    "        for word, _ in sorted(counter.items(), key=lambda pair:pair[1], reverse=True):\n",
    "            fw.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'simple-examples/data/vocab.txt'\n",
    "write_vocab(counter, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peek at vocab file, see if the words make sense...\n",
    "\n",
    "```bash\n",
    " head simple-examples/data/vocab.txt \n",
    "```\n",
    "\n",
    "This generates the following:\n",
    "```\n",
    "<unk>\n",
    "<eos>\n",
    "the\n",
    "N\n",
    "of\n",
    "to\n",
    "a\n",
    "in\n",
    "and\n",
    "'s\n",
    "```\n",
    "\n",
    "Next, we want to create a data pipeline, we would create a batch of src words and corresponding target words.\n",
    "\n",
    "Target words would be shifted right by one. Let us give a concrete example:\n",
    "\n",
    "**Sentence**: \"the cat sat on mat\"\n",
    "\n",
    "**Src_Words:**: ['the', 'cat', 'sat', 'on', 'mat']\n",
    "\n",
    "**Tgt_Words:**: ['cat', 'sat', 'on', 'mat', '<eos\\>']\n",
    "\n",
    "<img src=\"data_tx@2x.png\" alt=\"drawing\" width=\"300\"/>\n",
    "Let us begin by creating a vocab table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=121, shape=(), dtype=int64, numpy=10000>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file(vocab_file)\n",
    "vocab_table.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sentences_file, vocab_table, batch_size, eos=EOS):\n",
    "    #Create a Text Line dataset, which returns a string tensor\n",
    "    dataset = tf.data.TextLineDataset(sentences_file)\n",
    "    \n",
    "    #Convert to a list of words..\n",
    "    dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "    \n",
    "    #Create target words right shifted by one, append EOS, also return size of each sentence...\n",
    "    dataset = dataset.map(lambda words: (words, tf.concat([words[1:], [eos]], axis=0), tf.size(words)))\n",
    "    \n",
    "    #Lookup words, word->integer, EOS->1\n",
    "    dataset = dataset.map(lambda src_words, tgt_words, num_words: (vocab_table.lookup(src_words), vocab_table.lookup(tgt_words), num_words))\n",
    "    \n",
    "    #[None] -> src words, [None] -> tgt_words, [] length of sentence\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([None], [None], []))\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(train_file, vocab_table, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=172, shape=(32,), dtype=int32, numpy=\n",
       "array([24, 15, 11, 23, 34, 27, 23, 32,  9, 15,  8, 20, 21, 22, 31, 16, 19,\n",
       "       15, 20, 18, 32, 20, 38, 48, 17, 16, 12, 20, 12, 32, 20, 26],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out sample data!\n",
    "\n",
    "next(iter(dataset))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7: RNN Model (revisited)\n",
    "\n",
    "Now, that we have a way to load up data. Let us see how our RNN model behaves.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = Embedding(V=vocab_table.size(), d=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 48, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = word_embeddings(datum[0])\n",
    "word_vectors.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(h=128, cell='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_outputs = rnn(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num outputs: 48 Shape[0]: (32, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f'Num outputs: {len(rnn_outputs)} Shape[0]: {rnn_outputs[0].numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeroing out outputs past real sentence length!\n",
    "One problem, with our current RNN implementation is that it processes even past the sentence length. For example, length of sentence 0 is 24, but since longest sentence in first batch is of length 48. It returns outputs even past length 24. Let us confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=619, shape=(), dtype=int32, numpy=24>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=628, shape=(10,), dtype=float32, numpy=\n",
       "array([-0.06586117, -0.25426382,  0.09824807,  0.2871141 , -0.02431772,\n",
       "        0.00771092,  0.25113913,  0.10970695, -0.00239144,  0.0056459 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_outputs[40][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use static_rnn to deal with the zeroing problem... As you can see, it implements for loop by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticRNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(StaticRNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, word_vectors, num_words):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs, final_state = tf.nn.static_rnn(cell=self.cell, inputs=word_vectors_time, sequence_length=num_words, dtype=tf.float32)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1618, shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srnn = StaticRNN(h=256, cell='rnn')\n",
    "rnn_outputs = srnn(word_vectors, datum[2])\n",
    "rnn_outputs[40][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8: Language Model (Code)\n",
    "\n",
    "At each time step, we want to predict a probability distribution over the entire vocabulary\n",
    "\n",
    "Thus, we need to add an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.word_embedding = Embedding(V, d)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=V)\n",
    "        \n",
    "    def call(self, datum):\n",
    "        word_vectors = self.word_embedding(datum[0])\n",
    "        rnn_outputs_time = self.rnn(word_vectors, datum[2])\n",
    "        \n",
    "        #We want to convert it back to shape batch_size x TimeSteps x h\n",
    "        rnn_outputs = tf.stack(rnn_outputs_time, axis=1)\n",
    "        logits = self.output_layer(rnn_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(vocab_table.size(), 128, 128, 'rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the shape of logits returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape (32, 48, 10000)\n"
     ]
    }
   ],
   "source": [
    "logits = lm(datum)\n",
    "print(f'logits shape {logits.numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P9: Loss function\n",
    "\n",
    "* At each time step, RNN makes a prediction\n",
    "* More concretely it generated 10,000 (V) logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute loss by comparing the predictions against true labels. We will use Cross Entropy Loss.\n",
    "\n",
    "* Cross Entropy measures distance between two probability distributions $p$ and $q$.\n",
    "\n",
    "* When you have only one class as correct in true distribution. The Cross entropy simplifies to computing the loss of the target word!\n",
    "\n",
    "<img src=\"cross_entropy@2x.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "* You should never compute the target probability directly. Further as we have our labels with only correct index we would use sparse_softmax_cross_entropy_with_logits. We pass the logits to this method directly!\n",
    "\n",
    "Now let us get some intuition about the loss values..\n",
    "\n",
    "First let us compute cross entropy loss for a model that predicts each word equally likely. In this case the probability would be 1/V or 1/10000. This comes out to be 9.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2103405"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.log(1/10000).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see what is the loss for the first prediction on an untrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(vocab_table.size(), 128, 128, 'lstm')\n",
    "logits = lm(datum)\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.20561\n"
     ]
    }
   ],
   "source": [
    "print(loss[0][0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are not doing any better than making a random prediction! Which is fine as we have not trained our model!\n",
    "\n",
    "\n",
    "Next, we need to be careful about not adding any loss for the **padded values**.\n",
    "\n",
    "Let us check out length of first sentence, and see what are loss values past the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of first sentence:  24 Loss[24:]=[9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405\n",
      " 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405\n",
      " 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405 9.2103405\n",
      " 9.2103405 9.2103405 9.2103405]\n"
     ]
    }
   ],
   "source": [
    "print(f'Len of first sentence:  {datum[2][0]} Loss[{datum[2][0]}:]={loss[0][datum[2][0]:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually don't want to accumulate this loss! We will zero it out using sequence mask. Which creates a tensor of 0's and 1's as per the sequence length...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of first sentence:  24 Loss[24:]=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "loss = loss * mask\n",
    "print(f'Len of first sentence:  {datum[2][0]} Loss[{datum[2][0]}:]={loss[0][datum[2][0]:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4448, shape=(48,), dtype=float32, numpy=\n",
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we are training we would do it over a batch. In this case 32 sentences with many words in each sentence... Thus, we will compute an average loss over this batch\n",
    "\n",
    "We compute this by dividing total loss for the batch by total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 9.209136009216309\n"
     ]
    }
   ],
   "source": [
    "mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "loss = loss * mask\n",
    "avg_loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "print(f'Avg loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(model, datum):\n",
    "    logits = model(datum)\n",
    "    mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[1]) * mask\n",
    "    return tf.reduce_sum(loss) / tf.cast(tf.reduce_sum(datum[2]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10: Gradients Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grads_fun = tfe.implicit_value_and_gradients(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, gradients_value = loss_and_grads_fun(lm, datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.209136, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P11: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 50 Avg Loss: 7.341406345367432\n",
      "Epoch: 0 Step: 100 Avg Loss: 6.8079915046691895\n",
      "Epoch: 0 Step: 150 Avg Loss: 6.721277713775635\n",
      "Epoch: 0 Step: 200 Avg Loss: 6.436749458312988\n",
      "Epoch: 0 Step: 250 Avg Loss: 6.674849510192871\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function TFE_Py_TapeGradient> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/tfnlm/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_aggregate_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m   \"\"\"Aggregate gradients from multiple sources.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f03800002bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_grads_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfnlm/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m                                            \u001b[0mthis_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                                            \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                                            sources)\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfnlm/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[1;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 63\u001b[0;31m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function TFE_Py_TapeGradient> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "STATS_STEPS = 50\n",
    "\n",
    "lm = LanguageModel(vocab_table.size(), 128, 128, 'lstm')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    for step_num, datum in enumerate(dataset, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun(lm, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(gradients, global_step=tf.train.get_or_create_global_step())\n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the loss changed for the first batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=926612, shape=(), dtype=float32, numpy=6.6278515>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_grads_fun(lm, datum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old avg p_tgt: 0.00010003404299092957 New: 0.0013230025069788098\n"
     ]
    }
   ],
   "source": [
    "print(f'Old avg p_tgt: {np.exp(-9.21)} New: {np.exp(-loss_and_grads_fun(lm, datum)[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'global_step:0' shape=() dtype=int64, numpy=267>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P12: Saving your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = 'lm'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tfe.Checkpoint(optimizer=opt, model=lm, optimizer_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lm/ckpt-1'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
